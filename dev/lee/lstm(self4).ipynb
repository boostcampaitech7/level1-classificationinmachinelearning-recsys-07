{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 호출\n",
    "data_path: str = \"../data\"\n",
    "train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"train.csv\")).assign(_type=\"train\") # train 에는 _type = train \n",
    "test_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")).assign(_type=\"test\") # test 에는 _type = test\n",
    "submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")) # ID, target 열만 가진 데이터 미리 호출\n",
    "df: pd.DataFrame = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:02<00:00, 47.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "file_names: List[str] = [\n",
    "    f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "}\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "    _rename_rule = {\n",
    "        col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'target', '_type',\n",
      "       'hourly_market-data_coinbase-premium-index_coinbase_premium_gap',\n",
      "       'hourly_market-data_coinbase-premium-index_coinbase_premium_index',\n",
      "       'hourly_market-data_funding-rates_all_exchange_funding_rates',\n",
      "       'hourly_market-data_funding-rates_binance_funding_rates',\n",
      "       'hourly_market-data_funding-rates_bitmex_funding_rates',\n",
      "       'hourly_market-data_funding-rates_bybit_funding_rates',\n",
      "       'hourly_market-data_funding-rates_deribit_funding_rates',\n",
      "       ...\n",
      "       'hourly_network-data_hashrate_hashrate',\n",
      "       'hourly_network-data_supply_supply_total',\n",
      "       'hourly_network-data_supply_supply_new',\n",
      "       'hourly_network-data_tokens-transferred_tokens_transferred_total',\n",
      "       'hourly_network-data_tokens-transferred_tokens_transferred_mean',\n",
      "       'hourly_network-data_tokens-transferred_tokens_transferred_median',\n",
      "       'hourly_network-data_transactions-count_transactions_count_total',\n",
      "       'hourly_network-data_transactions-count_transactions_count_mean',\n",
      "       'hourly_network-data_utxo-count_utxo_count',\n",
      "       'hourly_network-data_velocity_velocity_supply_total'],\n",
      "      dtype='object', length=255)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close\n"
     ]
    }
   ],
   "source": [
    "for k in df.columns:\n",
    "    if 'close' in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11552, 23)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_dict: Dict[str, str] = {\n",
    "    \"ID\": \"ID\",\n",
    "    \"target\": \"target\",\n",
    "    \"_type\": \"_type\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_gap\": \"coinbase_premium_gap\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_index\": \"coinbase_premium_index\",\n",
    "    \"hourly_market-data_funding-rates_all_exchange_funding_rates\": \"funding_rates\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations\": \"long_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations_usd\": \"long_liquidations_usd\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations\": \"short_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations_usd\": \"short_liquidations_usd\",\n",
    "    \"hourly_market-data_open-interest_all_exchange_all_symbol_open_interest\": \"open_interest\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_ratio\": \"buy_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_sell_ratio\": \"buy_sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_volume\": \"buy_volume\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_ratio\": \"sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_volume\": \"sell_volume\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_active\": \"active_count\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_receiver\": \"receiver_count\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_sender\": \"sender_count\",\n",
    "    'hourly_network-data_hashrate_hashrate': \"hashrate_value\",\n",
    "    'hourly_network-data_transactions-count_transactions_count_total': 'transaction_count',\n",
    "    'hourly_network-data_velocity_velocity_supply_total': 'velocity_count',\n",
    "    'hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close' : 'close'\n",
    "}\n",
    "df = df[cols_dict.keys()].rename(cols_dict, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"buy_volume\"] - df[\"sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"buy_volume\"] - df[\"sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"buy_volume\"] / (df[\"sell_volume\"] + 1),\n",
    ")\n",
    "\n",
    "df['buy_volume_ma_3h'] = df['buy_volume'].rolling(window=3).mean()\n",
    "df['buy_volume_std_3h'] = df['buy_volume'].rolling(window=3).std()\n",
    "\n",
    "df['funding_rates_ma_3h'] = df['funding_rates'].rolling(window=3).mean()\n",
    "df['funding_rates_std_3h'] = df['funding_rates'].rolling(window=3).std()\n",
    "\n",
    "df['price_close_pct_change_1h'] = df['coinbase_premium_gap'].pct_change(periods=1)\n",
    "df['price_close_pct_change_3h'] = df['coinbase_premium_gap'].pct_change(periods=3)\n",
    "\n",
    "df['buy_volume_pct_change_1h'] = df['buy_volume'].pct_change(periods=1)\n",
    "df['buy_volume_pct_change_3h'] = df['buy_volume'].pct_change(periods=3)\n",
    "\n",
    "df['liquidation_diff_pct_change_3h'] = df['liquidation_diff'].pct_change(periods=3)\n",
    "\n",
    "df['buy_sell_ratio_pct_change_3h'] = df['buy_sell_ratio'].pct_change(periods=3)\n",
    "df['is_buy_dominant'] = (df['buy_sell_ratio'] > 1.0).astype(int)\n",
    "\n",
    "\n",
    "df['active_count_pct_change_3h'] = df['active_count'].pct_change(periods=3)\n",
    "df['active_count_pct_change_6h'] = df['active_count'].pct_change(periods=6)\n",
    "\n",
    "df['sender_receiver_ratio'] = df['sender_count'] / (df['receiver_count'] + 1)\n",
    "\n",
    "df['hashrate_pct_change_3h'] = df['hashrate_value'].pct_change(periods=3)\n",
    "\n",
    "df['transactions_count_pct_change_3h'] = df['transaction_count'].pct_change(periods=3)\n",
    "df['transactions_count_pct_change_6h'] = df['transaction_count'].pct_change(periods=6)\n",
    "\n",
    "df['velocity_pct_change_3h'] = df['velocity_count'].pct_change(periods=3)\n",
    "\n",
    "\n",
    "df['buy_volume_lag_1'] = df['buy_volume'].shift(1)\n",
    "df['buy_volume_lag_3'] = df['buy_volume'].shift(3)\n",
    "df['close_lag_1'] = df['close'].shift(1)\n",
    "df['close_lag_3'] = df['close'].shift(3)\n",
    "\n",
    "# category, continuous 열을 따로 할당해둠\n",
    "category_cols: List[str] = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\"]\n",
    "conti_cols: List[str] = [_ for _ in cols_dict.values() if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\", \"liquidation_diff\", \"liquidation_usd_diff\", \"volume_diff\",\n",
    "    \"buy_volume_ma_3h\", \"buy_volume_std_3h\", \"funding_rates_ma_3h\", \"funding_rates_std_3h\",\n",
    "    \"price_close_pct_change_1h\", \"price_close_pct_change_3h\", \"buy_volume_pct_change_1h\", \"buy_volume_pct_change_3h\",\n",
    "    \"liquidation_diff_pct_change_3h\", \"buy_sell_ratio_pct_change_3h\", \"is_buy_dominant\",\n",
    "    \"active_count_pct_change_3h\", \"sender_receiver_ratio\", \"hashrate_pct_change_3h\",\n",
    "    \"transactions_count_pct_change_3h\", \"velocity_pct_change_3h\",\n",
    "    'buy_volume_lag_1', 'buy_volume_lag_3', 'close_lag_1', 'close_lag_3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(\n",
    "    df: pd.DataFrame,\n",
    "    conti_cols: List[str],\n",
    "    intervals: List[int],\n",
    ") -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    연속형 변수의 shift feature 생성\n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "        conti_cols (List[str]): continuous colnames\n",
    "        intervals (List[int]): shifted intervals\n",
    "    Return:\n",
    "        List[pd.Series]\n",
    "    \"\"\"\n",
    "    df_shift_dict = [\n",
    "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
    "        for conti_col in conti_cols\n",
    "        for interval in intervals\n",
    "    ]\n",
    "    return df_shift_dict\n",
    "\n",
    "# 최대 24시간의 shift 피쳐를 계산\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat 하여 df 에 할당\n",
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
    "\n",
    "# 타겟 변수를 제외한 변수를 forwardfill, -999로 결측치 대체\n",
    "_target = df[\"target\"]\n",
    "df = df.ffill().fillna(-999).assign(target = _target)\n",
    "\n",
    "# _type에 따라 train, test 분리\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ID  target  coinbase_premium_gap  coinbase_premium_index  \\\n",
      "0  2023-01-01 00:00:00     2.0                 -9.86               -0.059650   \n",
      "1  2023-01-01 01:00:00     1.0                 -8.78               -0.053047   \n",
      "2  2023-01-01 02:00:00     1.0                 -9.59               -0.057952   \n",
      "3  2023-01-01 03:00:00     1.0                 -9.74               -0.058912   \n",
      "4  2023-01-01 04:00:00     2.0                -10.14               -0.061373   \n",
      "\n",
      "   funding_rates  long_liquidations  long_liquidations_usd  \\\n",
      "0       0.005049              0.012              197.51610   \n",
      "1       0.005049              0.000                0.00000   \n",
      "2       0.005049              0.000                0.00000   \n",
      "3       0.005067              0.593             9754.76891   \n",
      "4       0.006210              0.361             5944.43714   \n",
      "\n",
      "   short_liquidations  short_liquidations_usd  open_interest  ...  \\\n",
      "0               0.000                 0.00000   6.271344e+09  ...   \n",
      "1               0.712             11833.56104   6.288683e+09  ...   \n",
      "2               0.000                 0.00000   6.286796e+09  ...   \n",
      "3               0.000                 0.00000   6.284575e+09  ...   \n",
      "4               0.000                 0.00000   6.291582e+09  ...   \n",
      "\n",
      "   close_lag_3_14  close_lag_3_15  close_lag_3_16  close_lag_3_17  \\\n",
      "0          -999.0          -999.0          -999.0          -999.0   \n",
      "1          -999.0          -999.0          -999.0          -999.0   \n",
      "2          -999.0          -999.0          -999.0          -999.0   \n",
      "3          -999.0          -999.0          -999.0          -999.0   \n",
      "4          -999.0          -999.0          -999.0          -999.0   \n",
      "\n",
      "   close_lag_3_18  close_lag_3_19  close_lag_3_20  close_lag_3_21  \\\n",
      "0          -999.0          -999.0          -999.0          -999.0   \n",
      "1          -999.0          -999.0          -999.0          -999.0   \n",
      "2          -999.0          -999.0          -999.0          -999.0   \n",
      "3          -999.0          -999.0          -999.0          -999.0   \n",
      "4          -999.0          -999.0          -999.0          -999.0   \n",
      "\n",
      "   close_lag_3_22  close_lag_3_23  \n",
      "0          -999.0          -999.0  \n",
      "1          -999.0          -999.0  \n",
      "2          -999.0          -999.0  \n",
      "3          -999.0          -999.0  \n",
      "4          -999.0          -999.0  \n",
      "\n",
      "[5 rows x 1063 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_slicing(df: pd.DataFrame, window_size: int, step_size: int) -> pd.DataFrame:\n",
    "    sliced_data = []\n",
    "    \n",
    "    for start in range(0, len(df) - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "        window_df = df.iloc[start:end].copy()\n",
    "        # window_df['window_start_index'] = start\n",
    "        sliced_data.append(window_df)\n",
    "    \n",
    "    sliced_data_df = pd.concat(sliced_data, axis=0).reset_index(drop=True)\n",
    "    return sliced_data_df\n",
    "\n",
    "window_size = 24\n",
    "step_size = 12\n",
    "\n",
    "train_0_3 = train_df[train_df['target'].isin([0, 3])]\n",
    "train_1_2 = train_df[~train_df['target'].isin([0, 3])]\n",
    "\n",
    "augm_03 = window_slicing(train_0_3, window_size=window_size, step_size=step_size)\n",
    "\n",
    "#train_df_aug_window = pd.concat([train_1_2, augm_0_3], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://numpy.org/doc/stable/reference/generated/numpy.interp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def time_warping(df: pd.DataFrame, sigma: float = 0.2, seed: int = 42) -> pd.DataFrame:\n",
    "#     df_warped = df.copy()\n",
    "#     for col in df_warped.select_dtypes(include=[np.number]).columns:\n",
    "#         np.random.seed(seed)\n",
    "#         time_steps = np.arange(len(df_warped))\n",
    "#         random_warp = np.random.normal(loc=1.0, scale=sigma, size=len(time_steps))\n",
    "#         time_warped_col = np.interp(time_steps * random_warp, time_steps, df_warped[col].values)\n",
    "#         df_warped[col] = time_warped_col\n",
    "#     return df_warped\n",
    "\n",
    "# train_03 = train_df[train_df['target'].isin([0, 3])]\n",
    "# train_12 = train_df[~train_df['target'].isin([0, 3])]\n",
    "\n",
    "# df_warped = time_warping(train_03.drop(columns=['target', 'ID']))\n",
    "# df_warped = df_warped.reset_index(drop=True)\n",
    "# augm_warped = train_03.reset_index(drop=True) \n",
    "# df_warped['target'] = augm_warped['target'].values\n",
    "# df_warped['ID'] = augm_warped['ID'].values\n",
    "\n",
    "# augm_03 = pd.concat([augm_0_3, df_warped], axis=0)\n",
    "# augm_03 = pd.concat([train_df, augm_result], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_injection(df: pd.DataFrame, noise_level: float = 0.01, decay: float = 0.99, seed:int = 42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "    df_noisy = df.copy()\n",
    "    time_steps = np.arange(len(df_noisy))\n",
    "    for col in df_noisy.select_dtypes(include=[np.number]).columns:\n",
    "        noise = np.random.normal(0, noise_level*df_noisy[col].std(), size=df_noisy[col].shape)\n",
    "        decays = decay ** time_steps\n",
    "        df_noisy[col] = df_noisy[col] + noise * decays\n",
    "    \n",
    "    return df_noisy    \n",
    "\n",
    "train03 = train_df[train_df['target'].isin([0, 3])]\n",
    "\n",
    "df_noisy = noise_injection(augm_03.drop(columns=['target', 'ID']))\n",
    "df_noisy = df_noisy.reset_index(drop=True)\n",
    "augm03 = augm_03.reset_index(drop=True) \n",
    "df_noisy['target'] = augm03['target'].values\n",
    "df_noisy['ID'] = augm03['ID'].values\n",
    "\n",
    "#train_df_augm = df_noisy\n",
    "train_df_augm = pd.concat([train_1_2, df_noisy], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3048\n",
      "1545\n"
     ]
    }
   ],
   "source": [
    "print(len(df_noisy))  # df_noisy의 길이 확인\n",
    "print(len(train03)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4853 - loss: 1.0192 - val_accuracy: 0.5002 - val_loss: 0.7043\n",
      "Epoch 2/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5195 - loss: 0.6975 - val_accuracy: 0.5280 - val_loss: 0.7067\n",
      "Epoch 3/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5664 - loss: 0.6842 - val_accuracy: 0.5426 - val_loss: 0.6992\n",
      "Epoch 4/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6069 - loss: 0.6627 - val_accuracy: 0.5382 - val_loss: 0.7030\n",
      "Epoch 5/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6314 - loss: 0.6382 - val_accuracy: 0.5616 - val_loss: 0.6988\n",
      "Epoch 6/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6749 - loss: 0.5970 - val_accuracy: 0.5499 - val_loss: 0.7234\n",
      "Epoch 7/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7074 - loss: 0.5551 - val_accuracy: 0.5645 - val_loss: 0.7053\n",
      "Epoch 8/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7403 - loss: 0.5136 - val_accuracy: 0.5743 - val_loss: 0.7266\n",
      "Epoch 9/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7714 - loss: 0.4652 - val_accuracy: 0.5879 - val_loss: 0.7666\n",
      "Epoch 10/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8030 - loss: 0.4170 - val_accuracy: 0.5928 - val_loss: 0.7651\n",
      "Epoch 11/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8197 - loss: 0.3744 - val_accuracy: 0.6055 - val_loss: 0.7773\n",
      "Epoch 12/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8338 - loss: 0.3447 - val_accuracy: 0.6035 - val_loss: 0.8403\n",
      "Epoch 13/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8486 - loss: 0.3308 - val_accuracy: 0.6040 - val_loss: 0.8837\n",
      "Epoch 14/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8559 - loss: 0.3084 - val_accuracy: 0.6025 - val_loss: 0.9661\n",
      "Epoch 15/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8708 - loss: 0.2871 - val_accuracy: 0.6240 - val_loss: 0.9187\n",
      "Epoch 16/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8835 - loss: 0.2614 - val_accuracy: 0.6108 - val_loss: 1.0019\n",
      "Epoch 17/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8917 - loss: 0.2541 - val_accuracy: 0.6167 - val_loss: 1.0312\n",
      "Epoch 18/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8932 - loss: 0.2398 - val_accuracy: 0.6191 - val_loss: 1.0213\n",
      "Epoch 19/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9072 - loss: 0.2179 - val_accuracy: 0.6132 - val_loss: 1.0656\n",
      "Epoch 20/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9030 - loss: 0.2198 - val_accuracy: 0.6249 - val_loss: 1.1054\n",
      "Epoch 21/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9109 - loss: 0.2018 - val_accuracy: 0.6240 - val_loss: 1.0919\n",
      "Epoch 22/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9154 - loss: 0.1945 - val_accuracy: 0.6186 - val_loss: 1.1177\n",
      "Epoch 23/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9231 - loss: 0.1837 - val_accuracy: 0.6157 - val_loss: 1.1807\n",
      "Epoch 24/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9260 - loss: 0.1742 - val_accuracy: 0.6371 - val_loss: 1.2181\n",
      "Epoch 25/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9321 - loss: 0.1606 - val_accuracy: 0.6293 - val_loss: 1.2483\n",
      "Epoch 26/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9350 - loss: 0.1588 - val_accuracy: 0.6230 - val_loss: 1.2753\n",
      "Epoch 27/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9362 - loss: 0.1479 - val_accuracy: 0.6225 - val_loss: 1.2392\n",
      "Epoch 28/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9376 - loss: 0.1527 - val_accuracy: 0.6298 - val_loss: 1.3102\n",
      "Epoch 29/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9403 - loss: 0.1425 - val_accuracy: 0.6269 - val_loss: 1.3634\n",
      "Epoch 30/30\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9492 - loss: 0.1284 - val_accuracy: 0.6181 - val_loss: 1.3789\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "acc: 0.6181198246468582, auroc: 0.8608114616366551\n"
     ]
    }
   ],
   "source": [
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df_augm.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df_augm[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "x_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "x_train.fillna(-999, inplace=True)\n",
    "\n",
    "# Validation 데이터도 동일하게 처리\n",
    "x_valid.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "x_valid.fillna(-999, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "\n",
    "# LSTM 모델을 위한 데이터 reshape\n",
    "x_train_reshaped = x_train_scaled.reshape(x_train_scaled.shape[0], 1, x_train_scaled.shape[1])\n",
    "x_valid_reshaped = x_valid_scaled.reshape(x_valid_scaled.shape[0], 1, x_valid_scaled.shape[1])\n",
    "\n",
    "# LSTM 모델 구성\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(x_train_reshaped.shape[1], x_train_reshaped.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train_reshaped, y_train, epochs=30, batch_size=64, validation_data=(x_valid_reshaped, y_valid))\n",
    "\n",
    "# 예측 및 평가\n",
    "y_valid_pred_class = np.argmax(model.predict(x_valid_reshaped), axis=1)\n",
    "accuracy = accuracy_score(y_valid, y_valid_pred_class)\n",
    "auroc = roc_auc_score(y_valid, model.predict(x_valid_reshaped), multi_class=\"ovr\")\n",
    "\n",
    "print(f\"acc: {accuracy}, auroc: {auroc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_df_augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data target target\n",
      "2.0    0.357693\n",
      "1.0    0.345318\n",
      "3.0    0.154438\n",
      "0.0    0.142551\n",
      "Name: proportion, dtype: float64\n",
      "train data target target\n",
      "2    0.360292\n",
      "1    0.341657\n",
      "3    0.156151\n",
      "0    0.141900\n",
      "Name: proportion, dtype: float64\n",
      "valid data target target\n",
      "1    0.359961\n",
      "2    0.347297\n",
      "3    0.147589\n",
      "0    0.145153\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_ratio_total = train_df_augm[\"target\"].value_counts(normalize=True)\n",
    "\n",
    "target_ratio_train = y_train.value_counts(normalize=True)\n",
    "\n",
    "target_ratio_valid = y_valid.value_counts(normalize=True)\n",
    "\n",
    "print(\"total data target\", target_ratio_total)\n",
    "print(\"train data target\", target_ratio_train)\n",
    "print(\"valid data target\", target_ratio_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4820 - loss: 0.9588\n",
      "Epoch 2/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5238 - loss: 0.6937\n",
      "Epoch 3/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5608 - loss: 0.6842\n",
      "Epoch 4/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6011 - loss: 0.6658\n",
      "Epoch 5/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6256 - loss: 0.6414\n",
      "Epoch 6/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6646 - loss: 0.6126\n",
      "Epoch 7/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7014 - loss: 0.5632\n",
      "Epoch 8/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7289 - loss: 0.5160\n",
      "Epoch 9/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7670 - loss: 0.4736\n",
      "Epoch 10/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7852 - loss: 0.4370\n",
      "Epoch 11/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8102 - loss: 0.3878\n",
      "Epoch 12/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8321 - loss: 0.3651\n",
      "Epoch 13/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8417 - loss: 0.3335\n",
      "Epoch 14/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8487 - loss: 0.3287\n",
      "Epoch 15/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8700 - loss: 0.2889\n",
      "Epoch 16/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8794 - loss: 0.2767\n",
      "Epoch 17/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8888 - loss: 0.2582\n",
      "Epoch 18/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8926 - loss: 0.2455\n",
      "Epoch 19/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8937 - loss: 0.2378\n",
      "Epoch 20/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8959 - loss: 0.2362\n",
      "Epoch 21/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9109 - loss: 0.2093\n",
      "Epoch 22/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9138 - loss: 0.1980\n",
      "Epoch 23/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9142 - loss: 0.2033\n",
      "Epoch 24/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9134 - loss: 0.1938\n",
      "Epoch 25/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9244 - loss: 0.1761\n",
      "Epoch 26/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9248 - loss: 0.1842\n",
      "Epoch 27/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9306 - loss: 0.1685\n",
      "Epoch 28/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9215 - loss: 0.1744\n",
      "Epoch 29/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9394 - loss: 0.1534\n",
      "Epoch 30/30\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9345 - loss: 0.1566\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "x_train = train_df_augm.drop([\"target\", \"ID\"], axis=1)\n",
    "y_train = train_df_augm[\"target\"].astype(int)\n",
    "\n",
    "x_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "x_train.fillna(-999, inplace=True)\n",
    "\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "# LSTM 모델을 위한 데이터 reshape\n",
    "x_train_reshaped = x_train_scaled.reshape(x_train_scaled.shape[0], 1, x_train_scaled.shape[1])\n",
    "\n",
    "# LSTM 모델 다시 구성 및 학습\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(x_train_reshaped.shape[1], x_train_reshaped.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일 및 재학습\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "# 테스트 데이터 준비 및 예측\n",
    "x_test = test_df.drop([\"target\", \"ID\"], axis=1)\n",
    "\n",
    "# 무한대 값 및 결측치 처리\n",
    "x_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "x_test.fillna(-999, inplace=True)\n",
    "\n",
    "# 스케일링 및 reshape\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "x_test_reshaped = x_test_scaled.reshape(x_test_scaled.shape[0], 1, x_test_scaled.shape[1])\n",
    "\n",
    "# 예측\n",
    "y_test_pred = model.predict(x_test_reshaped)\n",
    "y_test_pred_class = np.argmax(y_test_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file 할당후 save \n",
    "submission_df = submission_df.assign(target = y_test_pred_class)\n",
    "submission_df.to_csv(\"self4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window slicing -> acc: 0.47735021919142717, auroc: 0.7850269840520915\n",
    "\n",
    "window slicing + noise injection acc: 0.5232853513971211, auroc: 0.8128695283409264\n",
    "\n",
    "window slicing 후 noise injection acc: 0.5428082191780822, auroc: 0.8135914100882286\n",
    "                                \n",
    "                                0.5801266439357039, auroc: 0.8530677400461201\n",
    "\n",
    "\n",
    "window slicing + time warping 후 noise injection -> 0.3963\n",
    "\n",
    "\n",
    "- 24시간 기준으로 시간 정보 추가\n",
    "- 요일 정보 추가 (7일로 나눠서)\n",
    "- 주말 여부 추가 (토요일(5), 일요일(6)을 주말로 간주)\n",
    "\n",
    "- 이전 윈도우와의 차이 계산\n",
    "-> 거래량 \n",
    "가격 변동등"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
