{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 호출\n",
    "data_path: str = \"../data\"\n",
    "train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"train.csv\")).assign(_type=\"train\") # train 에는 _type = train \n",
    "test_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")).assign(_type=\"test\") # test 에는 _type = test\n",
    "submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")) # ID, target 열만 가진 데이터 미리 호출\n",
    "df: pd.DataFrame = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:02<00:00, 48.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "file_names: List[str] = [\n",
    "    f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "}\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "    _rename_rule = {\n",
    "        col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'target', '_type',\n",
      "       'hourly_market-data_coinbase-premium-index_coinbase_premium_gap',\n",
      "       'hourly_market-data_coinbase-premium-index_coinbase_premium_index',\n",
      "       'hourly_market-data_funding-rates_all_exchange_funding_rates',\n",
      "       'hourly_market-data_funding-rates_binance_funding_rates',\n",
      "       'hourly_market-data_funding-rates_bitmex_funding_rates',\n",
      "       'hourly_market-data_funding-rates_bybit_funding_rates',\n",
      "       'hourly_market-data_funding-rates_deribit_funding_rates',\n",
      "       ...\n",
      "       'hourly_network-data_hashrate_hashrate',\n",
      "       'hourly_network-data_supply_supply_total',\n",
      "       'hourly_network-data_supply_supply_new',\n",
      "       'hourly_network-data_tokens-transferred_tokens_transferred_total',\n",
      "       'hourly_network-data_tokens-transferred_tokens_transferred_mean',\n",
      "       'hourly_network-data_tokens-transferred_tokens_transferred_median',\n",
      "       'hourly_network-data_transactions-count_transactions_count_total',\n",
      "       'hourly_network-data_transactions-count_transactions_count_mean',\n",
      "       'hourly_network-data_utxo-count_utxo_count',\n",
      "       'hourly_network-data_velocity_velocity_supply_total'],\n",
      "      dtype='object', length=255)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11552, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_dict: Dict[str, str] = {\n",
    "    \"ID\": \"ID\",\n",
    "    \"target\": \"target\",\n",
    "    \"_type\": \"_type\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_gap\": \"coinbase_premium_gap\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_index\": \"coinbase_premium_index\",\n",
    "    \"hourly_market-data_funding-rates_all_exchange_funding_rates\": \"funding_rates\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations\": \"long_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations_usd\": \"long_liquidations_usd\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations\": \"short_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations_usd\": \"short_liquidations_usd\",\n",
    "    \"hourly_market-data_open-interest_all_exchange_all_symbol_open_interest\": \"open_interest\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_ratio\": \"buy_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_sell_ratio\": \"buy_sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_volume\": \"buy_volume\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_ratio\": \"sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_volume\": \"sell_volume\",\n",
    "    # \"hourly_network-data_addresses-count_addresses_count_active\": \"active_count\",\n",
    "    # \"hourly_network-data_addresses-count_addresses_count_receiver\": \"receiver_count\",\n",
    "    # \"hourly_network-data_addresses-count_addresses_count_sender\": \"sender_count\",\n",
    "    # 'hourly_network-data_hashrate_hashrate': \"hashrate_value\",\n",
    "    # 'hourly_network-data_transactions-count_transactions_count_total': 'transaction_count',\n",
    "    # 'hourly_network-data_velocity_velocity_supply_total': 'velocity_count',\n",
    "    # 'hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close' : 'close'\n",
    "\n",
    "}\n",
    "df = df[cols_dict.keys()].rename(cols_dict, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"buy_volume\"] - df[\"sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"buy_volume\"] - df[\"sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"buy_volume\"] / (df[\"sell_volume\"] + 1),\n",
    ")\n",
    "\n",
    "df['buy_volume_ma_3h'] = df['buy_volume'].rolling(window=3).mean()\n",
    "df['buy_volume_std_3h'] = df['buy_volume'].rolling(window=3).std()\n",
    "\n",
    "df['funding_rates_ma_3h'] = df['funding_rates'].rolling(window=3).mean()\n",
    "df['funding_rates_std_3h'] = df['funding_rates'].rolling(window=3).std()\n",
    "\n",
    "df['price_close_pct_change_1h'] = df['coinbase_premium_gap'].pct_change(periods=1)\n",
    "df['price_close_pct_change_3h'] = df['coinbase_premium_gap'].pct_change(periods=3)\n",
    "\n",
    "df['buy_volume_pct_change_1h'] = df['buy_volume'].pct_change(periods=1)\n",
    "df['buy_volume_pct_change_3h'] = df['buy_volume'].pct_change(periods=3)\n",
    "\n",
    "df['liquidation_diff_pct_change_3h'] = df['liquidation_diff'].pct_change(periods=3)\n",
    "\n",
    "df['buy_sell_ratio_pct_change_3h'] = df['buy_sell_ratio'].pct_change(periods=3)\n",
    "df['is_buy_dominant'] = (df['buy_sell_ratio'] > 1.0).astype(int)\n",
    "\n",
    "\n",
    "# df['active_count_pct_change_3h'] = df['active_count'].pct_change(periods=3)\n",
    "# df['active_count_pct_change_6h'] = df['active_count'].pct_change(periods=6)\n",
    "\n",
    "# df['sender_receiver_ratio'] = df['sender_count'] / (df['receiver_count'] + 1)\n",
    "\n",
    "# df['hashrate_pct_change_3h'] = df['hashrate_value'].pct_change(periods=3)\n",
    "\n",
    "# df['transactions_count_pct_change_3h'] = df['transaction_count'].pct_change(periods=3)\n",
    "# df['transactions_count_pct_change_6h'] = df['transaction_count'].pct_change(periods=6)\n",
    "\n",
    "# df['velocity_pct_change_3h'] = df['velocity_count'].pct_change(periods=3)\n",
    "\n",
    "\n",
    "# df['buy_volume_lag_1'] = df['buy_volume'].shift(1)\n",
    "# df['buy_volume_lag_3'] = df['buy_volume'].shift(3)\n",
    "# df['close_lag_1'] = df['close'].shift(1)\n",
    "# df['close_lag_3'] = df['close'].shift(3)\n",
    "\n",
    "# category, continuous 열을 따로 할당해둠\n",
    "category_cols: List[str] = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\"]\n",
    "conti_cols: List[str] = [_ for _ in cols_dict.values() if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\", \"liquidation_diff\", \"liquidation_usd_diff\", \"volume_diff\",\n",
    "    \"buy_volume_ma_3h\", \"buy_volume_std_3h\", \"funding_rates_ma_3h\", \"funding_rates_std_3h\",\n",
    "    \"price_close_pct_change_1h\", \"price_close_pct_change_3h\", \"buy_volume_pct_change_1h\", \"buy_volume_pct_change_3h\",\n",
    "    \"liquidation_diff_pct_change_3h\", \"buy_sell_ratio_pct_change_3h\", \"is_buy_dominant\",\n",
    "    # \"active_count_pct_change_3h\", \"sender_receiver_ratio\", \"hashrate_pct_change_3h\",\n",
    "    # \"transactions_count_pct_change_3h\", \"velocity_pct_change_3h\",\n",
    "    # 'buy_volume_lag_1', 'buy_volume_lag_3', 'close_lag_1', 'close_lag_3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(\n",
    "    df: pd.DataFrame,\n",
    "    conti_cols: List[str],\n",
    "    intervals: List[int],\n",
    ") -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    연속형 변수의 shift feature 생성\n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "        conti_cols (List[str]): continuous colnames\n",
    "        intervals (List[int]): shifted intervals\n",
    "    Return:\n",
    "        List[pd.Series]\n",
    "    \"\"\"\n",
    "    df_shift_dict = [\n",
    "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
    "        for conti_col in conti_cols\n",
    "        for interval in intervals\n",
    "    ]\n",
    "    return df_shift_dict\n",
    "\n",
    "# 최대 24시간의 shift 피쳐를 계산\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat 하여 df 에 할당\n",
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
    "\n",
    "# 타겟 변수를 제외한 변수를 forwardfill, -999로 결측치 대체\n",
    "_target = df[\"target\"]\n",
    "df = df.ffill().fillna(-999).assign(target = _target)\n",
    "\n",
    "# _type에 따라 train, test 분리\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ID  target  coinbase_premium_gap  coinbase_premium_index  \\\n",
      "0  2023-01-01 00:00:00     2.0                 -9.86               -0.059650   \n",
      "1  2023-01-01 01:00:00     1.0                 -8.78               -0.053047   \n",
      "2  2023-01-01 02:00:00     1.0                 -9.59               -0.057952   \n",
      "3  2023-01-01 03:00:00     1.0                 -9.74               -0.058912   \n",
      "4  2023-01-01 04:00:00     2.0                -10.14               -0.061373   \n",
      "\n",
      "   funding_rates  long_liquidations  long_liquidations_usd  \\\n",
      "0       0.005049              0.012              197.51610   \n",
      "1       0.005049              0.000                0.00000   \n",
      "2       0.005049              0.000                0.00000   \n",
      "3       0.005067              0.593             9754.76891   \n",
      "4       0.006210              0.361             5944.43714   \n",
      "\n",
      "   short_liquidations  short_liquidations_usd  open_interest  ...  \\\n",
      "0               0.000                 0.00000   6.271344e+09  ...   \n",
      "1               0.712             11833.56104   6.288683e+09  ...   \n",
      "2               0.000                 0.00000   6.286796e+09  ...   \n",
      "3               0.000                 0.00000   6.284575e+09  ...   \n",
      "4               0.000                 0.00000   6.291582e+09  ...   \n",
      "\n",
      "   is_buy_dominant_14  is_buy_dominant_15  is_buy_dominant_16  \\\n",
      "0              -999.0              -999.0              -999.0   \n",
      "1              -999.0              -999.0              -999.0   \n",
      "2              -999.0              -999.0              -999.0   \n",
      "3              -999.0              -999.0              -999.0   \n",
      "4              -999.0              -999.0              -999.0   \n",
      "\n",
      "   is_buy_dominant_17  is_buy_dominant_18  is_buy_dominant_19  \\\n",
      "0              -999.0              -999.0              -999.0   \n",
      "1              -999.0              -999.0              -999.0   \n",
      "2              -999.0              -999.0              -999.0   \n",
      "3              -999.0              -999.0              -999.0   \n",
      "4              -999.0              -999.0              -999.0   \n",
      "\n",
      "   is_buy_dominant_20  is_buy_dominant_21  is_buy_dominant_22  \\\n",
      "0              -999.0              -999.0              -999.0   \n",
      "1              -999.0              -999.0              -999.0   \n",
      "2              -999.0              -999.0              -999.0   \n",
      "3              -999.0              -999.0              -999.0   \n",
      "4              -999.0              -999.0              -999.0   \n",
      "\n",
      "   is_buy_dominant_23  \n",
      "0              -999.0  \n",
      "1              -999.0  \n",
      "2              -999.0  \n",
      "3              -999.0  \n",
      "4              -999.0  \n",
      "\n",
      "[5 rows x 677 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_slicing(df: pd.DataFrame, window_size: int, step_size: int) -> pd.DataFrame:\n",
    "    sliced_data = []\n",
    "    \n",
    "    for start in range(0, len(df) - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "        window_df = df.iloc[start:end].copy()\n",
    "        # window_df['window_start_index'] = start\n",
    "        sliced_data.append(window_df)\n",
    "    \n",
    "    sliced_data_df = pd.concat(sliced_data, axis=0).reset_index(drop=True)\n",
    "    return sliced_data_df\n",
    "\n",
    "window_size = 24\n",
    "step_size = 12\n",
    "\n",
    "train_0_3 = train_df[train_df['target'].isin([0, 3])]\n",
    "train_1_2 = train_df[~train_df['target'].isin([0, 3])]\n",
    "\n",
    "augm_0_3 = window_slicing(train_0_3, window_size=window_size, step_size=step_size)\n",
    "augm_1_2 = window_slicing(train_1_2, window_size=window_size, step_size=step_size)\n",
    "\n",
    "\n",
    "#train_df_aug_window = pd.concat([train_1_2, augm_0_3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_injection(df: pd.DataFrame, noise_level: float = 0.01, decay: float = 0.99, seed:int = 42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "    df_noisy = df.copy()\n",
    "    time_steps = np.arange(len(df_noisy))\n",
    "    for col in df_noisy.select_dtypes(include=[np.number]).columns:\n",
    "        noise = np.random.normal(0, noise_level*df_noisy[col].std(), size=df_noisy[col].shape)\n",
    "        decays = decay ** time_steps\n",
    "        df_noisy[col] = df_noisy[col] + noise * decays\n",
    "    \n",
    "    return df_noisy    \n",
    "\n",
    "#train03 = train_df[train_df['target'].isin([0, 3])]\n",
    "\n",
    "df_noisy = noise_injection(augm_0_3.drop(columns=['target', 'ID']))\n",
    "df_noisy = df_noisy.reset_index(drop=True)\n",
    "augm03 = augm_0_3.reset_index(drop=True) \n",
    "df_noisy['target'] = augm03['target'].values\n",
    "df_noisy['ID'] = augm03['ID'].values\n",
    "\n",
    "df_noisy_1 = noise_injection(augm_1_2.drop(columns=['target', 'ID']))\n",
    "df_noisy_1 = df_noisy_1.reset_index(drop=True)\n",
    "augm12 = augm_1_2.reset_index(drop=True) \n",
    "df_noisy_1['target'] = augm12['target'].values\n",
    "df_noisy_1['ID'] = augm12['ID'].values\n",
    "\n",
    "augm = pd.concat([df_noisy, df_noisy_1], axis=0)\n",
    "augm_next = pd.concat([augm, augm_0_3], axis=0)\n",
    "\n",
    "\n",
    "train_df_augm = pd.concat([train_1_2, augm_next], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_noisy))  # df_noisy의 길이 확인\n",
    "# # print(len(train03)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[1]\tvalid_0's multi_logloss: 1.19197\n",
      "[2]\tvalid_0's multi_logloss: 1.15988\n",
      "[3]\tvalid_0's multi_logloss: 1.13302\n",
      "[4]\tvalid_0's multi_logloss: 1.10921\n",
      "[5]\tvalid_0's multi_logloss: 1.08747\n",
      "[6]\tvalid_0's multi_logloss: 1.06783\n",
      "[7]\tvalid_0's multi_logloss: 1.05132\n",
      "[8]\tvalid_0's multi_logloss: 1.03488\n",
      "[9]\tvalid_0's multi_logloss: 1.01995\n",
      "[10]\tvalid_0's multi_logloss: 1.00628\n",
      "[11]\tvalid_0's multi_logloss: 0.993142\n",
      "[12]\tvalid_0's multi_logloss: 0.981176\n",
      "[13]\tvalid_0's multi_logloss: 0.970067\n",
      "[14]\tvalid_0's multi_logloss: 0.958978\n",
      "[15]\tvalid_0's multi_logloss: 0.949044\n",
      "[16]\tvalid_0's multi_logloss: 0.939403\n",
      "[17]\tvalid_0's multi_logloss: 0.930447\n",
      "[18]\tvalid_0's multi_logloss: 0.921558\n",
      "[19]\tvalid_0's multi_logloss: 0.913447\n",
      "[20]\tvalid_0's multi_logloss: 0.905005\n",
      "[21]\tvalid_0's multi_logloss: 0.897267\n",
      "[22]\tvalid_0's multi_logloss: 0.889967\n",
      "[23]\tvalid_0's multi_logloss: 0.883023\n",
      "[24]\tvalid_0's multi_logloss: 0.876173\n",
      "[25]\tvalid_0's multi_logloss: 0.870259\n",
      "[26]\tvalid_0's multi_logloss: 0.86428\n",
      "[27]\tvalid_0's multi_logloss: 0.858266\n",
      "[28]\tvalid_0's multi_logloss: 0.852597\n",
      "[29]\tvalid_0's multi_logloss: 0.845349\n",
      "[30]\tvalid_0's multi_logloss: 0.839269\n",
      "acc: 0.645679235071261, auroc: 0.8793961990595993\n"
     ]
    }
   ],
   "source": [
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df_augm.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df_augm[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "unique_classes = [int(i) for i in np.unique(train_df_augm['target'])]\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=train_df_augm['target'])\n",
    "class_weights_dict = {i: class_weights[i] for i in unique_classes}\n",
    "\n",
    "sample_weights = train_df_augm['target'].map(class_weights_dict).values\n",
    "\n",
    "# lgb dataset\n",
    "train_data = lgb.Dataset(x_train, label=y_train, weight=sample_weights[x_train.index])\n",
    "valid_data = lgb.Dataset(x_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "# lgb params\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 4,\n",
    "    \"num_leaves\": 50,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 30,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "# lgb train\n",
    "lgb_model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_data,\n",
    "    valid_sets=valid_data,\n",
    ")\n",
    "\n",
    "# lgb predict\n",
    "y_valid_pred = lgb_model.predict(x_valid)\n",
    "weighted_predictions = y_valid_pred * np.array([class_weights_dict[i] for i in range(len(class_weights_dict))])\n",
    "\n",
    "y_valid_pred_class = np.argmax(weighted_predictions, axis=1)\n",
    "y_valid_pred_class = np.argmax(y_valid_pred, axis = 1)\n",
    "\n",
    "# score check\n",
    "accuracy = accuracy_score(y_valid, y_valid_pred_class)\n",
    "auroc = roc_auc_score(y_valid, y_valid_pred, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"acc: {accuracy}, auroc: {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_df_augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Feature  Importance\n",
      "518       price_close_pct_change_1h_5          91\n",
      "10                         buy_volume          43\n",
      "15                        volume_diff          37\n",
      "20                   buy_volume_ma_3h          35\n",
      "644   buy_sell_ratio_pct_change_3h_16          35\n",
      "12                        sell_volume          33\n",
      "570       buy_volume_pct_change_1h_11          33\n",
      "5                  short_liquidations          33\n",
      "13                   liquidation_diff          33\n",
      "525      price_close_pct_change_1h_12          30\n",
      "579       buy_volume_pct_change_1h_20          29\n",
      "539       price_close_pct_change_3h_3          28\n",
      "7                       open_interest          28\n",
      "28     liquidation_diff_pct_change_3h          27\n",
      "630    buy_sell_ratio_pct_change_3h_2          26\n",
      "642   buy_sell_ratio_pct_change_3h_14          26\n",
      "602       buy_volume_pct_change_3h_20          26\n",
      "551      price_close_pct_change_3h_15          26\n",
      "530      price_close_pct_change_1h_17          25\n",
      "566        buy_volume_pct_change_1h_7          24\n",
      "399                     volume_diff_1          24\n",
      "604       buy_volume_pct_change_3h_22          23\n",
      "25          price_close_pct_change_3h          23\n",
      "401                     volume_diff_3          23\n",
      "464              buy_volume_std_3h_20          23\n",
      "614  liquidation_diff_pct_change_3h_9          23\n",
      "634    buy_sell_ratio_pct_change_3h_6          22\n",
      "580       buy_volume_pct_change_1h_21          21\n",
      "26           buy_volume_pct_change_1h          21\n",
      "265                      buy_volume_5          21\n"
     ]
    }
   ],
   "source": [
    "importance = lgb_model.feature_importance()\n",
    "feature_names=train_df.drop([\"target\", \"ID\"], axis = 1).columns\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importance\n",
    "})\n",
    "\n",
    "# Sort by importance (optional)\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(feature_importance_df[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data target target\n",
      "2.0    0.396846\n",
      "1.0    0.383169\n",
      "3.0    0.114395\n",
      "0.0    0.105590\n",
      "Name: proportion, dtype: float64\n",
      "train data target target\n",
      "2    0.394713\n",
      "1    0.385150\n",
      "3    0.115527\n",
      "0    0.104610\n",
      "Name: proportion, dtype: float64\n",
      "valid data target target\n",
      "2    0.405376\n",
      "1    0.375248\n",
      "3    0.109868\n",
      "0    0.109507\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_ratio_total = train_df_augm[\"target\"].value_counts(normalize=True)\n",
    "\n",
    "target_ratio_train = y_train.value_counts(normalize=True)\n",
    "\n",
    "target_ratio_valid = y_valid.value_counts(normalize=True)\n",
    "\n",
    "print(\"total data target\", target_ratio_total)\n",
    "print(\"train data target\", target_ratio_train)\n",
    "print(\"valid data target\", target_ratio_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    }
   ],
   "source": [
    "# performance 체크후 전체 학습 데이터로 다시 재학습\n",
    "\n",
    "unique_classes = [int(i) for i in np.unique(train_df_augm['target'])]\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=train_df_augm['target'])\n",
    "class_weights_dict = {i: class_weights[i] for i in unique_classes}\n",
    "\n",
    "sample_weights = train_df_augm['target'].map(class_weights_dict).values\n",
    "\n",
    "x_train = train_df_augm.drop([\"target\", \"ID\"], axis = 1)\n",
    "y_train = train_df_augm[\"target\"].astype(int)\n",
    "train_data = lgb.Dataset(x_train, label=y_train, weight=sample_weights[x_train.index])\n",
    "lgb_model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb predict\n",
    "y_test_pred = lgb_model.predict(test_df.drop([\"target\", \"ID\"], axis = 1))\n",
    "weighted_predictions = y_test_pred * np.array([class_weights_dict[i] for i in range(len(class_weights_dict))])\n",
    "\n",
    "# 최댓값을 가진 클래스 선택\n",
    "y_test_pred_class = np.argmax(weighted_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file 할당후 save \n",
    "submission_df = submission_df.assign(target = y_test_pred_class)\n",
    "submission_df.to_csv(\"self6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  coinbase_premium_gap coinbase_premium_index funding_rates long_liquidations  \\\n",
      "0                  NaN                    NaN           NaN               NaN   \n",
      "1                  NaN                    NaN           NaN               NaN   \n",
      "2                  NaN                    NaN           NaN               NaN   \n",
      "3                  NaN                    NaN           NaN               NaN   \n",
      "4                  NaN                    NaN           NaN               NaN   \n",
      "\n",
      "  long_liquidations_usd short_liquidations short_liquidations_usd  \\\n",
      "0                   NaN                NaN                    NaN   \n",
      "1                   NaN                NaN                    NaN   \n",
      "2                   NaN                NaN                    NaN   \n",
      "3                   NaN                NaN                    NaN   \n",
      "4                   NaN                NaN                    NaN   \n",
      "\n",
      "  open_interest buy_ratio buy_sell_ratio  ... is_buy_dominant_15  \\\n",
      "0           NaN       NaN            NaN  ...                NaN   \n",
      "1           NaN       NaN            NaN  ...                NaN   \n",
      "2           NaN       NaN            NaN  ...                NaN   \n",
      "3           NaN       NaN            NaN  ...                NaN   \n",
      "4           NaN       NaN            NaN  ...                NaN   \n",
      "\n",
      "  is_buy_dominant_16 is_buy_dominant_17 is_buy_dominant_18 is_buy_dominant_19  \\\n",
      "0                NaN                NaN                NaN                NaN   \n",
      "1                NaN                NaN                NaN                NaN   \n",
      "2                NaN                NaN                NaN                NaN   \n",
      "3                NaN                NaN                NaN                NaN   \n",
      "4                NaN                NaN                NaN                NaN   \n",
      "\n",
      "  is_buy_dominant_20 is_buy_dominant_21 is_buy_dominant_22 is_buy_dominant_23  \\\n",
      "0                NaN                NaN                NaN                NaN   \n",
      "1                NaN                NaN                NaN                NaN   \n",
      "2                NaN                NaN                NaN                NaN   \n",
      "3                NaN                NaN                NaN                NaN   \n",
      "4                NaN                NaN                NaN                NaN   \n",
      "\n",
      "  target  \n",
      "0    2.0  \n",
      "1    1.0  \n",
      "2    1.0  \n",
      "3    1.0  \n",
      "4    2.0  \n",
      "\n",
      "[5 rows x 676 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train_data = train_data.data\n",
    "y_train_label = train_data.label\n",
    "\n",
    "# 2. 데이터프레임으로 변환하여 확인\n",
    "train_df = pd.DataFrame(X_train_data, columns=x_train.columns)\n",
    "train_df['target'] = y_train_label\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window slicing -> acc: 0.47735021919142717, auroc: 0.7850269840520915\n",
    "\n",
    "window slicing + noise injection acc: 0.5232853513971211, auroc: 0.8128695283409264\n",
    "\n",
    "window slicing 후 noise injection acc: 0.5428082191780822, auroc: 0.8135914100882286\n",
    "                                \n",
    "                                0.5801266439357039, auroc: 0.8530677400461201\n",
    "\n",
    "\n",
    "# 24시간 기준으로 시간 정보 추가\n",
    "# 요일 정보 추가 (7일로 나눠서)\n",
    "# 주말 여부 추가 (토요일(5), 일요일(6)을 주말로 간주)\n",
    "\n",
    "# 이전 윈도우와의 차이 계산\n",
    "-> 거래량 \n",
    "가격 변동등"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
